# Lesson 2: Exploration and control

# Tradeoff for RL Agents

- Learning agents need to tradeoff on two things:
    - ************************Exploitation************************: maximize the performance based on already learnt knowledge
    - **********************Exploration**********************: maximize the performance based on new information which can be useful
- We need to incorporate new information to then learn to make the best decisions.
- The best long-term strategy may require short-term sacrifices.

# The Multi-Armed Bandit

- A multi-armed bandit is a set of distributions $\{\mathcal{R}_a|a \in \mathcal{A} \}$
    - $\mathcal{A}$ is a known set of actions, or arms in the classical terminology.
    - $\mathcal{R}_a$ is a distribution of rewards, given an action $a$. What this means is that for each action $a$, there will be a separate probability distribution $\mathcal{R}_a$ from which we shall sample a reward in case that action is chosen.
    - A slot machine is a one-armed bandit as it takes your money away and gives you less than that. A multi-armed bandit can be thought of as a row of slot machines, each offering a different reward based on the actions you take. Can also be extended to recommender machines.
- At each timestep $t$ we select an action $a_t \in \mathcal{A}$.
- This yields a reward $R_t \sim R_{a_t}$.
- The goal is the maximize the cumulative reward $\sum_{i=1}^t R_i$.
- We do this by learning a policy which is a distribution over the set of actions $\mathcal{A}$.

### Code it out!

A snippet for a multi-armed bandit is shown below. Quite simply, we randomly select $|\mathcal{A}|$ points from a normal distribution which shall denote the means of each distribution $\mathcal{R}_a$. Each distribution is chosen as a normal distribution with $\sigma = 1$ as a simple demonstration.

```python
class MultiArmedBandit():
    def __init__(self, num_actions):
        self.num_actions = num_actions
        self.means_of_action_distributions = torch.normal(mean=0, std=1, size=(self.num_actions,))

    def sample(self, action_idx):
        mean_to_choose_from = self.means_of_action_distributions[action_idx].item()
        reward = torch.normal(mean=mean_to_choose_from, std=1.0, size=(1,))
        return reward.item()
```

# Value and Regret

- The ******************action value****************** for an action $a$ is defined as $q(a) = \mathbb{E}[R_t|a_t=a]$
- The optimal action value is simply
    
    $$
    v_* = \max_{a \in \mathcal{A}} q(a) = \max_{a \in \mathcal{A}} \mathbb{E}[R_t|a_t=a]
    $$
    
- Therefore the regret of an action $a$ can be defined as $\Delta_a = v_* - q(a)$. The regret of the optimal action is zero.
- The overall goal is to minimize the total regret given by
    
    $$
    L_t = \sum_{i=i}^t v_* - q(a_t) = \sum_{i=1}^t \Delta_{a_t}
    $$
    
- Maximizing the expected cumulative reward $\equiv$ minimizing the total regret

# Overview of Algorithms

- The algorithms can be used to tackle this problem are:
    - Greedy
    - $\epsilon$-greedy
    - UCB
    - Thompson sampling
    - Policy gradient
- The first three use ********************************************action value estimates******************************************** $Q_t(a) \approx q(a)$ where $q(a)$ is the true action value.
- A simple estimate is the average of timestep rewards for that action, given by
    
    $$
    N_t(a) = \sum_{i=1}^t \mathcal{I}(a_i=a)\\ Q_t(a) = \frac{\sum_{i=1}^t \mathcal{I}(a_i=a)R_i}{N_t(a)}
    $$
    
    where $N_t(a)$ is the count for that action $a$.
    

### Code it out!

Here we define a simple random strategy to pick actions to start us off.

```python
class RandomStrategy():
    def __init__(self, num_actions, total_timesteps):
        self.num_actions = num_actions
        self.total_timesteps = total_timesteps

    def sample(self, t):
        action = torch.randint(high=self.num_actions, size=(1,)).item()
        return action

    def update(self, t, stream):
        return
```

# Greedy Algorithm

- A simple way to use $Q_t(a)$ as a heuristic to select actions can be to greedily pick the action with the highest action value estimate. This is the Greedy Algorithm.
- This is formalized in $\pi(t) = \argmax_{a} Q_t(a)$
- While this greedy approach is intuitive, it can fall short when we consider the following scenario:
    - Let us take the first action randomly. For this action we receive a high value of reward. Let us denote this action by $a^*$.
    - Since $Q_1(a^*)$ is currently the highest, we pick this action again, however it yields $0$ rewards for the next step and for all subsequent steps.
    - In this case, we shall still keep choosing action $a^*$ despite of poor rewards, and *continuously exploit where it may be useful to explore.*

### Code it out!

Here we initial a table to hold the values of the function $Q_t(\cdot)$. At each step we update its values from a stream of actions and rewards.

```python
class GreedyStrategy():
    def __init__(self, num_actions, total_timesteps):
        self.num_actions = num_actions
        self.total_timesteps = total_timesteps

        self.actions = [x for x in range(num_actions)]
        self.action_estimates_table = torch.zeros(total_timesteps, num_actions)
        self.action_counts_table = torch.zeros(total_timesteps, num_actions)

    def sample(self, t):
        if t == 0:
            return random.choice(self.actions)
        else:
            return self.action_estimates_table[t].argmax().item()

    def update(self, t, stream):
        action_counts = [0 for _ in range(self.num_actions)]
        Q_num_for_this_timestep = [0 for _ in range(self.num_actions)]

        for action_idx in self.actions:
            for i in range(t):
                if stream.actions[i] == action_idx:
                    action_counts[action_idx] += 1
                    Q_num_for_this_timestep[action_idx] += stream.rewards[i]

            if action_counts[action_idx] == 0:
                self.action_estimates_table[t][action_idx] = 0

            else:
                self.action_estimates_table[t][action_idx] = Q_num_for_this_timestep[action_idx] / action_counts[action_idx]

            self.action_counts_table[t][action_idx] = action_counts[action_idx]
```

# Epsilon Greedy

- Instead of deterministically taking the highest estimated action like in the Greedy Algorithm, we can introduce stochasticity in this process.
- If the action $a = \argmax Q_t(a)$ then we pick this action with probability $1 - \epsilon$.
- Otherwise, we randomly pick an action with probability $\epsilon / |\mathcal{A}|$.
- Here, $\epsilon$ is a scalar which is updated at each timestep. It can be observed that varying $\epsilon$ from $0.99 \rightarrow 0.0$ shall encourage exploration at initial timesteps and exploitation at later timesteps.

### Code it out!

This utilizes the previous snippet for the Greedy Algorithm, but incorporates the stochastic componet of $\epsilon$. We add $\epsilon / |\mathcal{A}|$ even when the action is of the highest estimate in case there is a tie between the actions.

```python
class EpsilonGreedyStrategy():
    def __init__(self, num_actions, total_timesteps, epsilon):
        self.num_actions = num_actions
        self.actions = [x for x in range(self.num_actions)]
        self.total_timesteps = total_timesteps
        self.epsilon = self.initialize_epsilon()

        self.action_estimates_table = torch.zeros(total_timesteps, num_actions)
        self.action_counts_table = torch.zeros(total_timesteps, num_actions)
        self.probabilities = [self.epsilon / self.num_actions for _ in range(self.num_actions)]

    def initialize_epsilon(self):
        return 0.99

    def step(self):
        self.epsilon -= 0.01

    def sample(self, t):
        max_idx = self.action_estimates_table[t].argmax().item()

        for action_idx in self.actions:
            if action_idx == max_idx:
                self.probabilities[action_idx] = (1 - self.epsilon) + self.epsilon / self.num_actions
            else:
                self.probabilities[action_idx] = self.epsilon / self.num_actions

        if t == 0:
            return random.choice(self.actions)
        else:
            return self.probabilities.index(max(self.probabilities))

    def update(self, t, stream):
        action_counts = [0 for _ in range(self.num_actions)]
        Q_num_for_this_timestep = [0 for _ in range(self.num_actions)]

        for action_idx in self.actions:
            for i in range(t):
                if stream.actions[i] == action_idx:
                    action_counts[action_idx] += 1
                    Q_num_for_this_timestep[action_idx] += stream.rewards[i]

            if action_counts[action_idx] == 0:
                self.action_estimates_table[t][action_idx] = 0

            else:
                self.action_estimates_table[t][action_idx] = Q_num_for_this_timestep[action_idx] / action_counts[action_idx]

            self.action_counts_table[t][action_idx] = action_counts[action_idx]
        
        self.step()
```

---

In the next blog, we shall look at the ****************Policy Gradient**************** Algorithm as the mathematical formulation and derivation of its process shall be better explained in a standalone blog.