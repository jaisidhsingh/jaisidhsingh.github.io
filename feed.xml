<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jaisidhsingh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jaisidhsingh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-11T10:59:12+00:00</updated><id>https://jaisidhsingh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Simple Toy Model Bridging HTSR &amp;amp; Alpha-REQ</title><link href="https://jaisidhsingh.github.io/blog/2026/htsrlink/" rel="alternate" type="text/html" title="A Simple Toy Model Bridging HTSR &amp;amp; Alpha-REQ"/><published>2026-02-10T00:00:00+00:00</published><updated>2026-02-10T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2026/htsrlink</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2026/htsrlink/"><![CDATA[<h2 id="tldr">TLDR</h2> <blockquote> <p>Generalization in neural networks has been shown to be correlated with one phenomenon expressed in two venues: how strongly singular values decay in (i) weights (Martin and Mahoney’s HTSR theory) and (ii) features (Agarwal’s $\alpha$-REQ). This blog connects the two approaches for toy linear models to show how HTSR may give rise to $\alpha$-REQ.</p> </blockquote> <p><br/></p> <h2 id="introduction">Introduction</h2> <p>Recent advances in deep learning theory have used spectral analysis as a tool to study generalization of neural networks. On one hand, we have Heavy-Tailed Self-Regularization (HTSR) theory, which analyzes a network’s weights to demonstrate that in well-trained networks, singular values of weights follow a power-law decay. On the other hand, $\alpha$-ReQ analyzes activations or features to show that high-quality representations also follow a specific spectral decay that correlates with generalization.</p> <p>While the two approaches are proposed and used separately, it is plain that properties of weights and features are coupled by design. Hence, this post explores the connection between HTSR and $\alpha$-REQ through fundamental linear algebra on a toy linear model $Y = XW$.</p> <ol> <li>First, we will quickly relate the condition number of a matrix to the decay of the singular values.</li> <li>Then we will use our linear model to look at how the singular value decay/conditioning of the weights $W$ can impact the features $Y$.</li> </ol> <p>The result in this post is mainly a corollary of those in the vast literature on signal propagation in neural networks. Our aim here is to simply yet mathematically relate two interesting emergences of geometry in distinct spaces, i.e., weights and features.</p> <p><br/></p> <h2 id="linking-singular-value-decay-to-condition-number">Linking singular value decay to condition number</h2> <p>Intuitively, one can see that a steeper decay in the singular value spectrum of a matrix should correspond to a higher condition number and vice versa, as a flatter spectrum implies that the minimum singular value is still reasonably close to the maximum singular value. Let us prove this formally.</p> <p>We start with a matrix $A \in \mathbb{R}^{n \times m}$ that has full rank, i.e., $A^T A$ is symmetric and positive semi-definite (PSD). Then, we can write the condition number of $A$ as</p> \[\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = \frac{\sqrt{\lambda_{\max}(A^T A)}}{\sqrt{\lambda_{\min}(A^T A)}}\] <p>Then, assuming that the eigenvalues of $A^T A$ follow a powerlaw such that $\lambda = c_0 u^{-\alpha}$ where $u$ is a scalar and $c_0$ is a fixed constant, we can write</p> \[\lambda_{\max} = c_0 (u_{\max})^{-\alpha} \quad \text{and} \quad \lambda_{\min} = c_0 (u_{\min})^{-\alpha}\] <p>Since $u_{\min} = m \ u_{\max}$, we can substitute this in the expression for $\kappa(A)$ to find</p> \[\kappa(A) = \frac{\sqrt{\lambda_{\max}}}{\sqrt{\lambda_{\min}}} = \frac{\sqrt{c_0 (u_{\min} / m)^{-\alpha}}}{\sqrt{c_0 (u_{\min})^{-\alpha}}} = m^{\alpha/2}\] <p>This result confirms our intuition: how fast the singular value spectrum of $A$ decays is exponentially proportional to the $\kappa(A)$. Accordingly, this result will let us connect HTSR with $\alpha$-REQ cleanly in the following.</p> <p><br/></p> <h2 id="toy-model-to-bridge-htsr-and-alpha-req">Toy model to bridge HTSR and $\alpha$-REQ</h2> <p>Given a data matrix $X \in \mathbb{R}^{n \times d}$, where $n$ is the number of data points ad $d$ is the dimension of each sample’s features, we are interested in a linear transform of $X$ by weights $W \in \mathbb{R}^{d \times m}$ to compute $Y \in \mathbb{R}^{n \times m}$ as</p> \[Y = X W\] <p>According to HTSR theory, the powerlaw fit of the decay of singular values of $W$ should reduce as the weights are updated throughout training, and $\alpha$-REQ shows how the same happens for features $Y$. We will now derive a link between the two observations.</p> <p>For any two compatible matrices $A$ and $B$, the condition number of their matrix multiplication is bounded as</p> \[\kappa(AB) \leq \kappa(A) \cdot \kappa(B)\] <p>This result simply follows from the nice properties of singular values: given some singular values arranged in descending order like $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_N \geq 0$, we have for $A$ and $B$</p> \[\sigma_N(A)\sigma_i(B) \leq \sigma_i(AB) \leq \sigma_1(A)\sigma_i(B) \quad i=1,2,\dots,N\] <p>Corresondingly, $\sigma_{\max}(AB) \leq \sigma_{\max}(A) \ \sigma_{\max}(B)$ and $\sigma_{\text{min}}(AB) \geq \sigma_{\text{min}}(A) \ \sigma_{\text{min}}(B)$ give the above result for the condition number of a matrix multiplication. Then, we can see that</p> \[\kappa(Y) \leq \kappa(X) \cdot \kappa(W) = C_{\text{data}} \cdot \kappa(W)\] <p>Here $\kappa(X)$ is denoted by a fixed scalar $C_{\text{data}}$ that assumes the data matrix to be of full rank and fixed. Finally, using the relationship between condition number and powerlaw fit coefficient for singular value decay, we can see the bound</p> \[c_{Y} \cdot m^{\alpha_Y/2} \leq (C_{\text{data}} \cdot c_{W}) \cdot m^{\alpha_W/2} \implies m^{\alpha_Y/2} \leq \frac{C_{\text{data}} \cdot c_{W}}{c_{Y}} \cdot m^{\alpha_W/2}\] <p>This shows that as the powerlaw fit of the singular value decay $\alpha_W$ for weights $W$ reduces, so should the maximum value of the corresponding powerlaw fit $\alpha_Y$ for features $Y$.</p> <p><br/></p> <h2 id="discussion">Discussion</h2> <p>What happens when the $\kappa(X)$ is not stationary? This is precisely the case for deep neural networks, as the inputs to the linear projection evolve due to non-linear activation functions and due to cross-activation interactions during training. While one can continually substitute the fixed condition number of the inputs to the neural network, non-linearities complicate analytical expressions of the relationship of HTSR and $\alpha$-REQ.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Bounding conditioning of features with that of weights]]></summary></entry><entry><title type="html">Muon and Manifold Versions</title><link href="https://jaisidhsingh.github.io/blog/2026/muon/" rel="alternate" type="text/html" title="Muon and Manifold Versions"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2026/muon</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2026/muon/"><![CDATA[<blockquote> <p>In this blog, we take a close look at Muon, try to understand it, and then move towards Manifold Muon. We cover all of the math required to implement and understand these optimizers, after which we will implement Manifold Muon in <code class="language-plaintext highlighter-rouge">optax</code>. This implementation will be used in a pull request to the official <code class="language-plaintext highlighter-rouge">optax</code> library to add Manifold Muon to the many optimizers in <code class="language-plaintext highlighter-rouge">optax</code>. Additionally, we do a tiny CIFAR-10 speed-run ourselves to see the improvements of Manifold Muon over the original.</p> </blockquote> <h2 id="muon">Muon</h2> <p>Muon is an adaptive optimizer designed by <a href="xxx">Keller Jordan</a> that has recently become quite popular, especially in the speed-running community of deep learning. What makes this optimizer special in comparison to the reigning champion <a href="decoupled optimisation">AdamW</a> is its emphasis on <em>orthogonality of updates</em>. More specifically, Muon searches for the nearest semi-orthogonal matrix to the update to a particular matrix, via something called a Newton-Schulz (NS) iteration. We’ll explain all of this precisely with the math below.<br/></p> <h3 id="theory-and-design">Theory and Design</h3> <p>Before we begin understanding Muon in detail, a quick note: Muon only operates on 2D weight matrices. All scalar and vector parameters as well as the input and output layers (for e.g. the embedding look-up layer in transformers and the language modelling head) of a model should be optimized by AdamW and <em>not</em> Muon. For convolutional layers, it is possible to optimize them via Muon, however, the last 3 dimensions of the weight tensor must be collapsed into one dimension so that the resulting tensor is 2D.</p> <p>Now, given a weight matrix $\theta_{t-1} \in \mathbb{R}^{n\times m}$ at timestep $t$ of training, we compute its gradient w.r.t. the loss $\mathcal{L}$ as $g_t = \nabla_{\theta_{t-1}}\mathcal{L}(\theta_{t-1}), \ g_t \in \mathbb{R}^{n\times m}$. The weights $\theta$ are then updated to $\theta_{t+1}$ as</p> \[\theta_{t} = \theta_{t-1} - \eta \ u_t\] <p>where $\eta$ is the learning rate and $u_t \in \mathbb{R}^{n\times m}$ is the update that we compute as follows.</p> <ol> <li>Momentum $\mu$ is applied to the gradient as $m_t = \mu m_{t-1} + g_t, \ m_0 = 0$.</li> <li>Next, NS iteration on $m_t$ for $5$ steps finally yields the update $u_t$. The NS iteration is explained below.<br/></li> </ol> <h4 id="newton-schulz-iteration">Newton-Schulz Iteration</h4> <p>The NS iteration is used to approximately orthogonalize the pre-update ($m_t$) by solving the constrained optimization problem</p> \[\operatorname{Orthogonalize}(X) = \arg\min_Z \| Z - X \|_F \qquad \text{s.t.} \qquad \text{either} \quad Z^\top Z = I \quad \text{or}\quad ZZ^\top = I\] <p>Note that the constraint specifies either $Z^\top Z = I$ or $ZZ^\top = I$. This makes the minimizer a <em>semi-orthogonal matrix</em> and equivalently means that if the singular value decomposition (SVD) of $X$ is $U\Sigma V^\top$, then the SVD of $\operatorname{Orthogonalize}(X)$ will be $UV^\top$ (the matrix of singular values $\Sigma$ of the semi-orthogonal matrix is just the identity matrix $I$). Having established this, we now show the NS iteration algorithm below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ns_iteration</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">Wrong no. of dimensions in input</span><span class="sh">"</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="mf">3.4445</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7750</span><span class="p">,</span> <span class="mf">2.0315</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
  
  <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">T</span>
  
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">y</span> <span class="o">@</span> <span class="n">y</span><span class="p">.</span><span class="n">T</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">A</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">A</span> <span class="o">@</span> <span class="n">A</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">B</span> <span class="o">@</span> <span class="n">y</span>
  
  <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">T</span>
  
  <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <p>Let us write one step of this iteration to gain a better understanding of why it orthogonalizes its input. Let $X \in \mathbb{R}^{p\times q}$ be the input matrix. Then,</p> \[Z = aX + b(XX^\top)X + c(XX^\top)^2X\] \[=(aI + bXX^\top + c (XX^\top)^2)X\] <p>Now substituting the SVD of $X$ as $U\Sigma V^\top$, we get</p> \[=(a\cdot I + b\cdot U\Sigma^2 U^\top + c\cdot U\Sigma^4 U)U\Sigma V^\top\] \[=U(a\cdot\Sigma +b\cdot\Sigma^3 + c\cdot\Sigma^5 )V^\top\] <p>Clearly, one step of the NS iteration yields a matrix whose singular values are a quintic ($5^{\text{th}}$ order with only odd powers) polynomial $\rho(x) = ax + bx^3 + cx^5$. Applying this iteration for $T$ steps would then apply the polynomial $T$ times (we denote this by $\rho^T(x)$) on the singular values. Since the singular values all lie in $[0, 1]$ due to the normalization of the input, we then only need to choose the polynomial coefficients such that as $T \to \infty, \rho^T(x) \to 1 \ \forall x \in [0, 1]$. This would make all singular values of the resultant matrix tend to $1$, thereby making it orthogonal.</p> <p>That’s it! Choosing good coefficients of the polynomials gets us to an appreciably fast iteration, where the following constraints must be kept in mind while choosing them:</p> <ol> <li>$a$ must be large as $\rho’(0)=a$. This implies that $a$ controls the rate of convergence for small initial singular values.</li> <li>For every $x \in [0, 1]$ we want to converge to the singular value interval $[1-\epsilon, 1+\epsilon]$ for some $\epsilon &gt; 0$ as $T \to \infty$ so that the result of the NS iteration is not far from its input.</li> </ol> <p>While one can experiment with different ways to solve this optimization problem, the creator of Muon arrives at the ones given in the algorithm-code-block by employing a post-hoc gradient-based approach.<br/></p> <h4 id="why-orthogonalize">Why orthogonalize?</h4> <p>Keller Jordan provides a concise and intuitive explanation for why orthogonality is desirable: normally, when optimizing with Adam/AdamW, we find that weight matrices have high condition numbers, implying only a few dominant directions within the matrix transformation. Orthoginalization is thus speculated to amplify the effect of these damped “rare” directions in order to speed-up learning.</p> <p>Let’s try and understand the need for orthogonality in more detail. First, we begin by emphasizing that Muon explicitly enforces <em>orthogonal updates</em>. This is different from explicitly enforcing orthogonal weights. Now, recall that for a given timestep</p> \[\theta_{t} = \theta_{t-1} - \eta \ u_t \ ; \qquad u_t^\top u_t = I\] <p>Let us write the change in the output of the linear transform</p> \[\Delta h = \theta_t x - \theta_{t-1}x\] <p>given some input $x$. Clearly, $\Delta h = \eta \ u_t \ x$ and so</p> \[\|\Delta h\|^2 = \eta^2 \ (u_t \ x)^\top (u_t \ x) = \eta^2 \ x^\top u_t^\top u_t \ x\] \[= \eta^2 \ x^\top x = \eta^2 \ \|x\|^2\] <p>This result is quite important as it shows us that the singular value structure of the gradient does not govern the update. We can recall that in default SGD, the effective learning rate along a singular value $\sigma_i(g_t)$’s direction is given by $\eta_{\text{effective}, i} = \eta \ \sigma_i(g_t)$. With Muon, all directions move by the same effective step size, as Muon enforces $\sigma_i(g_t) \to 1$. This removes bias towards a few high variance directions, preventing the update matrix from becoming of high condition number and correspondingly, optimization from collapsing to a rank-deficit regime. This is especially important for deep networks as singular value spectra gets sharper across depth and so optimization essentially locks into a few modes. Also, one learning rate works for all layers.</p> <p>The Thinking Machines’ blog also notes the same. The goal in neural network optimization is to have well-behaved updates: not too large and not too small. Various normalizations attempt to realise this desire. Relevantly, Muon normalizes the impact of the gradient across its singular value spectra, thus making updates more well-behaved and controllable.<br/></p> <h3 id="cifar-10-speedrun">CIFAR-10 speedrun</h3> <p>asdf<br/></p> <h2 id="manifold-muon">Manifold Muon</h2> <p><br/></p> <h3 id="theory-and-design-1">Theory and design</h3> <p>asdf<br/></p> <h3 id="optax-implementation">Optax implementation</h3> <p>asdf<br/></p> <h3 id="cifar-10-speedrun-1">CIFAR-10 speedrun</h3> <p>asdf<br/></p> <h2 id="references">References</h2> <ol> <li><a href="https://thinkingmachines.ai/blog/modular-manifolds">Thinking Machines’ Manifold Muon blog</a></li> <li><a href="https://sdbuchanan.com/blog/manifold-muon/">Sam D. Buchanan’s blog</a></li> <li><a href="https://kellerjordan.github.io/posts/muon/">Keller Jordan’s blog</a></li> <li><a href="https://github.com/KellerJordan/Muon">Keller Jordan’s Muon GitHub repo</a></li> <li><a href="https://github.com/google-deepmind/optax/blob/main/optax/contrib/__init__.py">Google DeepMind’s Optax GitHub repo</a></li> <li><a href="https://github.com/thinking-machines-lab/manifolds/tree/main">Thinking Machine’s Manifolds GitHub repo</a></li> </ol>]]></content><author><name></name></author><category term="optimisation"/><category term="ml"/><summary type="html"><![CDATA[A dive into Muon, manifold constraints, and optax.]]></summary></entry><entry><title type="html">Grokking fast and slow</title><link href="https://jaisidhsingh.github.io/blog/2025/grokking/" rel="alternate" type="text/html" title="Grokking fast and slow"/><published>2025-12-15T00:00:00+00:00</published><updated>2025-12-15T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/grokking</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/grokking/"><![CDATA[<h3 id="tldr">TLDR</h3> <p>Grokking, or delayed generalisation in neural networks, can</p> <ul> <li>help us understand when and how (potentially dangerous) capabilities can emerge in neural networks</li> <li>circumvent data limitations by showing that the right inductive biases can lead to strong networks</li> <li>be <em>partially</em> explained by current theories of lazy-to-feature learning, however, exceptions exist as shown below.</li> </ul> <h3 id="what-is-grokking">What is grokking?</h3> <p>Grokking is when a neural network generalises, i.e, achieves zero test error, significantly after already achieving near zero train error. It has been extensively studied in small networks on toy math problems, however, it’s not restricted to these settings.</p> <h3 id="what-motivates-grokking-research">What motivates grokking research?</h3> <p>Firstly, observing a phenomenon like grokking suggests the possibility that a neural network can suddenly attain strong skills in potentially dangerous contexts, all while practitioners never expected it to do so (due to the train-test gap before the grok). This point has become central for machine learning researchers interested in safety of neural networks. Thus, grokking offers controllable toy models that can help investigate how and why neural networks might exhibit rapidly emerging capabilities.</p> <p>Second, it is notable that grokking is popularly seen under specific train-test data splits, often where training data is low. This becomes relevant for machine learning practitioners: in the event that data becomes the bottleneck to creating more capable models (some argue this is already happening), grokking can offer hope that strong generalisation can eventually emerge given the correct inductive biases are baked into the models.</p> <h3 id="why-grokking-happens-weight-decay-vs-lazy-to-feature-learning">Why grokking happens: weight decay v/s lazy-to-feature learning</h3> <p>Why a network groks on a particular task is still a mystery, although a number of studies have tried to answer this. Popularly, one collection of these studies approached grokking from the perspective of weight decay. When the train loss is nearly zero, parameter updates are largely driven by weight decay, which still allow significant exploration in the parameters to induce the sudden discovery of the rule (the steep ascent). Newer works along these lines have also formulated grokking as weight-norm minimisation in the post-memorization phase.</p> <p>Another theory, however, has emerged which models grokking as the transition from a “lazy-like” learning regime to feature learning regime. More specifically, it argues that in the memorization phase, the network learns lazily, i.e, its neural tangent kernel (NTK) evolves rather negligbly until the network discovers the generalisable rule, after which the network enters a “rich feature learning” phase where the NTK evolves quite variably.</p> <p>While both theories present compelling evidence for their core hypotheses, one cannot explain grokking <em>in every architecture and task</em> using them. Grokking profiles can vary quite strongly depending on one’s model, task, and optimisation setup. For instance, a counter-example to the argument for general solutions being low in weight-norm, one can perform a simple polynomial regression experiment and find increasing weight-norm across training, likely because the task requires large degrees-of-freedom in the weights. As for the lazy-to-feature learning interpretation, this holds only for MLP models following an MSE objective under SGD optimisers. More complex models, objectives, and optimisers do not exhibit near-static NTK evolution across training.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Whats, whys, and what-ifs of grokking]]></summary></entry><entry><title type="html">Celebration is the secret</title><link href="https://jaisidhsingh.github.io/blog/2025/celebration/" rel="alternate" type="text/html" title="Celebration is the secret"/><published>2025-11-20T00:00:00+00:00</published><updated>2025-11-20T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/celebration</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/celebration/"><![CDATA[<p>This semester, I’ve been taking a course at my university taught by someone I’ve worked with and admired for a while now. My admiration for the instructor stems from two main reasons.</p> <p>First, the instructor often breaks down seemingly complex scenarios into very digestible ones by removing any complexity not relevant to the question at hand, and does so with absolutely zero ego or intimidation. Second, every single interaction with him is a positive one, such that I truly feel that my day gets better every time we meet.</p> <p>The second reason is something I’ve been thinking about for quite some time now. What trait or habit underlies the instructor’s “make every interaction a positive one” superpower?</p> <p>Turns out the answer to this question was in front of me all this time, and was revealed to me at the start of the sixth lecture of the course: <em>celebration</em>. The instructor celebrates every moment of the lecture. He celebrates every question one would ask. He speaks with a perpetual smile on his face that unmistakably comes from pure enjoyment of the moment at hand.</p> <p>That’s why it feels so good to interact with him, because he celebrates my questions, my research, my confusion, etc. The celebration and positivity is contagious: who’d dislike someone that genuinely enjoys the chance to include and to be included? I also feel this is strongly relevant to why there’s no perceivable ego to him despite the fact that he’s a genius (at least in my book).</p> <p>This way of being is pervasive across philosophy. Krishna teaches in the Gita: when attachment, ego, and anxiety fall away, worldly engagements become light and almost “play-like”. Similarly, Abhinavagupta describes creation itself as a celebratory overflow of ānanda. It’s one thing to read such ideas but another to witness them embodied, and for that I’m very grateful.</p>]]></content><author><name></name></author><category term="reflection"/><category term="reflection"/><summary type="html"><![CDATA[Inspiration from someone I look up to]]></summary></entry><entry><title type="html">New POVs on hypernetworks</title><link href="https://jaisidhsingh.github.io/blog/2025/hypernets/" rel="alternate" type="text/html" title="New POVs on hypernetworks"/><published>2025-08-20T00:00:00+00:00</published><updated>2025-08-20T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/hypernets</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/hypernets/"><![CDATA[<h3 id="tldr">TLDR</h3> <blockquote> <p>we can interpret hypernetworks as (i) generators of implicit neural representations, (ii) quantifiers of functional similarities between neural networks.</p> </blockquote> <hr/> <p>I’ve worked on hypernetworks (neural networks that parameterize other neural networks) for a little while, that has led to <a href="https://openreview.net/forum?id=dyRHRxcgXX">a workshop paper at ICLR 2025</a> and <a href="https://arxiv.org/pdf/2507.10015">a main conference paper at EMNLP 2025</a>. While working on these papers, I’ve had the time to think about hypernetworks in a couple of fascinating ways that I’ve described below.</p> <p><br/></p> <h3 id="hypernetworks-as-generators-of-inrs">Hypernetworks as generators of INRs</h3> <p><em>Implicit neural representations</em> (INRs) are a clever and rather under-appreciated class of representations. Contrary to the popular approach of predicting representations, INRs <em>are</em> representations themselves: given $f(x)=y$, if we train a neural network $g_\theta$ to predict $y$ from $x$, the parameters of this network $\theta$ <em>implicitly represent</em> the function $f(\cdot)$.</p> <p>Hence, when learning several functions $f_1, \dots, f_n$, we can decide to predict weights (or INRs) for $f_i$, using a generating function $H(\cdot)$ as $H(i) = \theta_i$ such that $g_{\theta_i}(x) = f_i(x)$. From the deep learning perspective, this generating function $H(\cdot)$ is called a hypernetwork.</p> <p>This ability to conditionally generate INRs is what makes hypernetworks strongly applicable in physics-informed machine learning. In particular, we can see how the process is just another formuation of partial differential equations (PDEs), which denote time-varying functions of space $\nu_t(x)$ in differential form.</p> <p>Hypernetworks can very well predict spatial functions given a timestep: $H(t) = \theta_t$ such that $g_{\theta_t}(x) = \nu_t(x)$ as they can conditionally generate an INR corresponding to the function at tilmestep $t$, and can be used to forecast PDEs. Indeed they’ve been used to do so in a very interesting <a href="https://arxiv.org/abs/2209.14855">NeurIPs paper</a> that I’ve presented in a tutorial <a href="https://jaisidhsingh.github.io/assets/DINo_continuous_pde_forecasting_with_INRs.html">here</a>.</p> <p><br/></p> <h3 id="hypernetworks-as-quantifiers-of-functional-similarity">Hypernetworks as quantifiers of functional similarity</h3> <p>Let’s say that we wish to know the functional similarity between two encoders $f_A, f_B: \mathbb{R}^{m} \to \mathbb{R}^{d}$. The most straightforward way to do this would be to collect a stack of encodings for $N$ inputs for each encoder and use a similarity function like Centered-Kernel-Alignment (CKA) on these features: \(s_1 = CKA(O_A, O_B) \ ; \quad O_A, O_B \in \mathbb{R}^{N\times d}\) where $s_1 \in [0, 1]$ is a score denoting the functional similarity between the two models, and $O_A, O_B$ are the stacks of encoders for $N$ inputs.</p> <p>However, let’s think of another way to quantify the functional similarity between $f_A$ and $f_B$, specifically, by using a hypernetwork $H$ that predicts linear classifiers $W_A, W_B \in \mathbb{R}^{d\times k}$ on top of the two encoders $f_A, f_B$. Here, $k$ is the number of classes.</p> <p>Writing, the composite function $g_j(\bullet) = W_j f_j(\bullet), \ j \in {A, B}$ to classify the inputs, we outline the scheme that let’s the hypernetwork depict functional similarity between $f_A$ and $f_B$ as follows:</p> <ul> <li> <p>Given a dataset $\mathcal{D}$ and the current state of the hypernetwork’s parameters $\phi^0$, predict \(W^{0}_A \text{ , } W^{0}_B = H_{\phi^0}(c_A) \text{ , } H_{\phi^0}(c_B)\)</p> </li> <li>Obtain the classification loss of $g^0_A$ and $g^0_B$ on $\mathcal{D}$ as $\ell^0_A$ and $\ell^0_B$.</li> <li>Train the hypernetwork’s parameters $\phi$ only on encoder $f_A$ as \(\phi^1 \leftarrow \phi^0 - \eta \nabla_{\phi^0}\ell^0_A\)</li> <li>Then predict \(W^{1}_A \text{ , } W^{1}_B = H_{\phi^1}(c_A) \text{ , } H_{\phi^1}(c_B)\)</li> <li>Obtain the classification loss of $g^1_A, g^1_B \text{ as } \ell^1_A, \ell^1_B$.</li> <li>Then, the magnitude of $\Delta = \ell^0_B - \ell^1_B$ depicts the functional similarity between encoders $f_A$ and $f_B$.</li> </ul> <p>In other words, if training the hypernetwork <em>only</em> using network A lowers the loss of network B as well, then network A and B can be called functionally similar.</p> <hr/> <p>The above interpretations of hypernetworks are avenues that I think are rather under-utilised in the current literature, and can offer creative and potentially powerful ways of modulating neural networks. I’d be happy to know what you think of the above, or about parameter prediction in general. Hit my up for a chat on <a href="https://x.com/jaisidhsingh"> X/twitter</a> if you like.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Hypernetworks from the POV of INRs and functional similarity]]></summary></entry></feed>