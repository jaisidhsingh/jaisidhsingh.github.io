<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jaisidhsingh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jaisidhsingh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-01T09:50:06+00:00</updated><id>https://jaisidhsingh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Celebration is the secret</title><link href="https://jaisidhsingh.github.io/blog/2025/celebration/" rel="alternate" type="text/html" title="Celebration is the secret"/><published>2025-11-20T00:00:00+00:00</published><updated>2025-11-20T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/celebration</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/celebration/"><![CDATA[<p>This semester, I‚Äôve been taking a course at my university taught by someone I‚Äôve worked with and admired for a while now. My admiration for the instructor stems from two main reasons.</p> <p>First, the instructor often breaks down seemingly complex scenarios into very digestible ones by removing any complexity not relevant to the question at hand, and does so with absolutely zero ego or intimidation. Second, every single interaction with him is a positive one, such that I truly feel that my day gets better every time we meet.</p> <p>The second reason is something I‚Äôve been thinking about for quite some time now. What trait or habit underlies the instructor‚Äôs ‚Äúmake every interaction a positive one‚Äù superpower?</p> <p>Turns out the answer to this question was in front of me all this time, and was revealed to me at the start of the sixth lecture of the course: <em>celebration</em>. The instructor celebrates every moment of the lecture. He celebrates every question one would ask. He speaks with a perpetual smile on his face, the kind of smile that unmistakably comes from pure enjoyment of the moment at hand.</p> <p>That‚Äôs why it feels so good to interact with him, because he celebrates my questions, my research, my confusion, etc. The celebration and positivity is contagious: who‚Äôd dislike someone that genuinely enjoys the chance to include and to be included? I also feel this is strongly relevant to why there‚Äôs no perceivable ego to him despite the fact that he‚Äôs a genius (at least in my book).</p> <p>This way of being is pervasive across philosophy. Krishna teaches in the Gita: when attachment, ego, and anxiety fall away, worldly engagements become light and almost ‚Äúplay-like‚Äù. Similarly, Abhinavagupta describes creation itself as a celebratory overflow of ƒÅnanda. It‚Äôs one thing to read such ideas, it‚Äôs another to witness them embodied. For that I‚Äôm very grateful.</p>]]></content><author><name></name></author><category term="reflection"/><category term="reflection"/><summary type="html"><![CDATA[Inspiration from someone I look up to]]></summary></entry><entry><title type="html">Ultra-Scale Playbook vol-3 - DeepSpeed ZeRO</title><link href="https://jaisidhsingh.github.io/blog/2025/uspb3/" rel="alternate" type="text/html" title="Ultra-Scale Playbook vol-3 - DeepSpeed ZeRO"/><published>2025-09-07T00:00:00+00:00</published><updated>2025-09-07T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/uspb3</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/uspb3/"><![CDATA[<h2 id="zero-redundancy-optimiser">Zero Redundancy Optimiser</h2> <p>TLDR: Instead of naive data parallelism,</p> <ul> <li>partition the optimizer states, gradients, and parameters across the DP dimension,</li> <li>while still allowing computation with the full parameter set. Can require more communnication between DP ranks (think: overlapping).</li> </ul> <h3 id="3-stages-of-zero">3 stages of ZeRO</h3> <ol> <li>ZeRO-1: only partition optimiser states</li> <li>ZeRO-2: partition optimiser states + gradients</li> <li>ZeRO-3: partition optimiser states + gradients + params</li> </ol> <p>Given the model parameter count $N$, mixed precision training with Adam dictates the following memory usage:</p> <ul> <li>params <code class="language-plaintext highlighter-rouge">bf16</code> = $2N$</li> <li>grads <code class="language-plaintext highlighter-rouge">bf16</code> = $2N$, if we want to accumulate grad in <code class="language-plaintext highlighter-rouge">fp32</code> then $4N$</li> <li>optimiser states <code class="language-plaintext highlighter-rouge">fp32</code> = $4N + 4N$, and parameters <code class="language-plaintext highlighter-rouge">fp32</code> = $4N$</li> </ul> <p>For efficiency let‚Äôs keep grad-accumulation in <code class="language-plaintext highlighter-rouge">bf16</code> and so total memory usage becomes $2N + 2N + 12N$. Now given data parallel degree $N_d$,</p> <ul> <li><strong>Baseline:</strong> $2N + 2N + 12N$ across each DP rank</li> <li><strong>ZeRO-1:</strong> $2N + 2N + 12N/N_d$</li> <li><strong>ZeRO-2:</strong> $2N + (2N + 12N)/N_d$</li> <li><strong>ZeRO-3:</strong> $(2N + 2N + 12N)/N_d$</li> </ul> <h3 id="zero-1-only-shard-optimiser-states">ZeRO-1: only shard optimiser states</h3> <ul> <li>Each DP rank forward passes with the same full set of <code class="language-plaintext highlighter-rouge">bf16</code> params but on a different microbatch of data.</li> <li>Each DP rank back-props with the same full set of <code class="language-plaintext highlighter-rouge">bf16</code> grads but on a different microbatch of data.</li> <li>Since each DP rank stores only $1/N_d$ of the optimiser states, and we want to back-prop on all the data via a mean/sum reduction, we perform a <code class="language-plaintext highlighter-rouge">reduce_scatter</code> on the gradients, i.e., $grad_k^j \leftarrow \sum_l grad^j_l$. This accumulates the gradient of each $N/N_d$-th parameter chunk on all the data $\equiv \nabla_w \ell(X) = \sum_{i=1}^n \nabla_w \ell(X_i)$.</li> <li>Now that we have each $N/N_d$-th parameter chunk accumulated across the data, we perform an optimiser step using only $1/N_d$ states $\implies$ $1/N_d$ <code class="language-plaintext highlighter-rouge">fp32</code> updated params, converted to $1/N_d$ <code class="language-plaintext highlighter-rouge">bf16</code> params.</li> <li>At this point, each DP rank has $1/N_d$ updated <code class="language-plaintext highlighter-rouge">bf16</code> params. It now receives the other $(N_d-1)/N_d$ fraction of params from the other DP ranks via an <code class="language-plaintext highlighter-rouge">all_gather</code> operation.</li> <li>Now each DP rank has the same full set of updated <code class="language-plaintext highlighter-rouge">bf16</code> params, ready for the next step of training.</li> <li>Final memory footprint: $2N + 2N + 12N / N_d$.</li> </ul> <p>üí° To update each chunk during <code class="language-plaintext highlighter-rouge">reduce_scatter</code>, only that chunk (across different microbatches) is needed per machine.</p> <p><strong>What motivates ZeRO-2:</strong> Why not accumulate a chunk on all data during back-prop, and then only store the grads required for the optimiser step? That eliminates the need to always store all the grads.</p> <h3 id="zero-2-shard-optimiser-states--grad">ZeRO-2: shard optimiser states + grad</h3> <p>Only perform <code class="language-plaintext highlighter-rouge">reduce_scatter</code> during back-prop. Now only $1/N_d$-th of the gradients are needed in memory, freeing up memory and giving us a much better memory footprint of $2N + (2N + 12N)/N_d$.</p> <p><strong>What motivates ZeRO-3:</strong> Distributing the params across DP ranks can make forward pass possible by doing an <code class="language-plaintext highlighter-rouge">all_gather</code> for each microbatch per DP rank. Think of it this way: we temporarily ‚Äústitch‚Äù all the shards of a layer together, forward pass a microbatch through it, then flush the gathered shards to keep only $1/N_d$ params in memory.</p> <h3 id="zero-3-shard-everything">ZeRO-3: shard everything</h3> <ul> <li><strong>Forward pass:</strong> distribute the <code class="language-plaintext highlighter-rouge">bf16</code> param set across DP ranks $\implies$ $2N / N_d$ memory on each rank. Given a microbatch, use <code class="language-plaintext highlighter-rouge">all_gather</code> to stitch a sharded layer together, apply the layer on the microbatch, and flush the other gathered param shards from memory.</li> <li><strong>Back-prop:</strong> use <code class="language-plaintext highlighter-rouge">all_gather</code> to stitch sharded params together, compute gradient for gathered params per microbatch, and then <code class="language-plaintext highlighter-rouge">reduce_scatter</code> to accumulate all microbatches into each chunk.</li> <li>Final memory footprint: $(2N + 2N + 12N)/N_d$.</li> </ul> <p>üí° ZeRO-3 requires $2 \cdot \text{num_layers} -1$ additional calls to <code class="language-plaintext highlighter-rouge">all_gather</code> w.r.t ZeRO-2, and each comes with a small base latency. Also, we gather all the shards once in the forward pass and once during back-prop, incurring a communication tax of $N$ each time. Adding another communication tax of $N$ from the <code class="language-plaintext highlighter-rouge">reduce_scatter</code> called during back-prop, our total communication cost is $3N$ compared to $2N$ in ZeRO-2.</p> <p>üí° While this may seem like a lot of overhead due to communication, in practice we use <em>prefetching</em> to compensate for this: simply <code class="language-plaintext highlighter-rouge">all_gather</code> weights for the next layer when we forward pass through the current layer. Similarly, <code class="language-plaintext highlighter-rouge">all_gather</code> weights of the previous layer while back-propping through the current layer.</p> <p><strong>When will this fail:</strong> when our DP dimension exceeds 512, the communication overhead will become too large due to <em>ring latency</em>, and our overlap will fail. Need to think of something else at those scales.</p>]]></content><author><name></name></author><category term="scaling"/><category term="ml"/><summary type="html"><![CDATA[On the three kinds of ZeRO used with Data Parallelism]]></summary></entry><entry><title type="html">Ultra-Scale Playbook vol-2 - Data Parallelism</title><link href="https://jaisidhsingh.github.io/blog/2025/uspb2/" rel="alternate" type="text/html" title="Ultra-Scale Playbook vol-2 - Data Parallelism"/><published>2025-09-03T00:00:00+00:00</published><updated>2025-09-03T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/uspb2</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/uspb2/"><![CDATA[<h2 id="data-parallelism-dp">Data Parallelism (DP)</h2> <p>TLDR: Instead of accumulating gradients for $k$ steps on a single GPU</p> <ul> <li>use $k$ GPUs that each have a copy of the model</li> <li>each GPU processes a unique microbatch in parallel</li> </ul> <h3 id="core-challenge-of-dp">Core challenge of DP</h3> <p>The main issue with DP is the communication overhead added when communicating with all $k$ GPUs to accumulate the gradient. Overlapping gradient computatio and communication addresses this well, provided the number of GPUs we perform DP over are less than 512.</p> <p>Let‚Äôs understand this in more detail:</p> <ul> <li>DP uses multiple instances of our model on multiple GPUs such that each micro-batch ($bs/k$) can be processed in parallel.</li> <li>It is natural to think that after all the parallel forward+backward passes are done, we need to wait for the GPUs to synchronise (communicate) so that the gradients stored across all GPUs are averaged via <code class="language-plaintext highlighter-rouge">all-reduce</code>, however, this is a BIG NO NO.</li> <li>We don‚Äôt want our GPUs to be idle while they communicate, so we overlap computation with inter-GPU communication: gradients of layers can be gathered and summed before those of earlier layers, so by the time we compute the gradient of early layers, we already have a reduced sum of all previous gradients.</li> </ul> <h3 id="bucketed-dp">Bucketed-DP</h3> <p>GPU operations on large tensors are more efficient than many operations on small tensors. So, we can <em>bucket</em> layer-wise gradients and launch a single <code class="language-plaintext highlighter-rouge">all-reduce</code> for all gradients within one bucket.</p> <h3 id="optimising-gpu-syncs">Optimising GPU syncs</h3> <p>By default, all GPUs synchronise after each backward pass. However, a single reduce at the end of the $k^{th}$ step would have the same effect as reducing at each of the $k$ gradient accumulation steps. This gives us room for further optimisation.</p> <p><strong>Core idea:</strong></p> <ul> <li>Reduce (with bucketed overlap) only once before <code class="language-plaintext highlighter-rouge">optimiser.step()</code> gets called.</li> <li>PyTorch typically solves this with a <code class="language-plaintext highlighter-rouge">model.no_sync()</code> decorator that disables gradient sync on backward passes that don‚Äôt need reduction.</li> </ul> <h3 id="ddp-the-full-package">DDP (the full package)</h3> <p>The complete package of DP with these 3 optimisations is sometimes referred to as DDP (distributed data parallelism), where our final global batch size becomes $bs = mbs \times gradacc \times k$. Note that:</p> <ul> <li>$mbs$ is the microbatch size (samples shown per GPU)</li> <li>$gradacc$ is the number of gradient accumulation steps (accumulate gradients per GPU across $k$ GPUs.)</li> </ul> <p>Typically, we maximise $k$ due to its parallel nature over maximising $gradacc$ as gradient accumulation is sequential. Practically, gradient accumulation is added on top of DP to meet a global batch size.</p>]]></content><author><name></name></author><category term="scaling"/><category term="ml"/><summary type="html"><![CDATA[Parallelising data batches across GPUs with Data Parallelism]]></summary></entry><entry><title type="html">Ultra-Scale Playbook vol-1 - Single GPU</title><link href="https://jaisidhsingh.github.io/blog/2025/uspb1/" rel="alternate" type="text/html" title="Ultra-Scale Playbook vol-1 - Single GPU"/><published>2025-09-01T00:00:00+00:00</published><updated>2025-09-01T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/uspb1</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/uspb1/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Optimally scaling LLM training across multiple GPUs requires finding the right tradeoffs between memory, computational efficiency, and communication overhead.</p> <p>It‚Äôs crucial to understand that each component, i.e., memory, compute-efficiency, and communication overhead, needs to be tuned carefully.</p> <h3 id="communication">Communication</h3> <p>Communication among nodes requires GPUs to be idle. That‚Äôs detrimental to overall efficiency because we want our GPUs to spend the most amount of time computing. That requires optimising intra-node/inter-node bandwidth usage, data transfers, and waiting for/syncing GPUs.</p> <h3 id="memory">Memory</h3> <p>Talking about memory, storing <em>all</em> activations is quadratically expensive with $seq$. Instead, store only expensive-to-recompute activations, discard the rest, and then recompute the discarded ones in the backward pass. We call this <strong>gradient checkpointing</strong> or <strong>recompilation</strong>, that trades off compute for memory</p> <p>Attention scores and matrices are an example of recomputable activations. <code class="language-plaintext highlighter-rouge">FlashAttention</code> natively integrates recomputing attention values/matrices in the backward pass out-of-the-box.</p> <p><strong>Consideration wrt GPU architecture:</strong> GPUs have limited high-speed memory, and accessing memory is typically slower than performing computations.</p> <h3 id="compute-efficiency">Compute-efficiency</h3> <p>Activations still scale linearly with $bs$, which is why we use <em>gradient accumulation</em>: sum up (avg in practice) gradients across $k$ passes (with batch size $bs/k$) and only then take an optimiser step. Requires keeping buffers for gradients that persist in a training step.</p> <p>However, gradient accumulation does not mean free lunch, as we get more computational overhead from lower per-pass batch size $bs/k$. Taking an optimiser step once every $k$ batches does not alleviate the cost of more forward &amp; backward passes.</p> <p>What‚Äôs neat is that we can parallelise the $k$ forward + backward passes. They aren‚Äôt correlated in any manner and just need to be summed to accumulate the gradient. However, that needs more VRAM.</p>]]></content><author><name></name></author><category term="uspb"/><category term="ml"/><summary type="html"><![CDATA[Kick-off to a series of notes on LLM scaling]]></summary></entry><entry><title type="html">New POVs on hypernetworks</title><link href="https://jaisidhsingh.github.io/blog/2025/hypernets/" rel="alternate" type="text/html" title="New POVs on hypernetworks"/><published>2025-08-20T00:00:00+00:00</published><updated>2025-08-20T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/blog/2025/hypernets</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/2025/hypernets/"><![CDATA[<h3 id="tldr">TLDR</h3> <blockquote> <p>we can interpret hypernetworks as (i) generators of implicit neural representations, (ii) quantifiers of functional similarities.</p> </blockquote> <hr/> <p>I‚Äôve worked on hypernetworks (neural networks that parameterize other neural networks) for a little while, that has led to <a href="https://openreview.net/forum?id=dyRHRxcgXX">a workshop paper at ICLR 2025</a> and <a href="https://arxiv.org/pdf/2507.10015">a main conference paper at EMNLP 2025</a>. While working on these papers, I‚Äôve had the time to think about hypernetworks in a couple of fascinating ways that I‚Äôve described below.</p> <p><br/></p> <h3 id="hypernetworks-as-generators-of-inrs">Hypernetworks as generators of INRs</h3> <p><em>Implicit neural representations</em> (INRs) are a clever and rather under-appreciated class of representations. Contrary to the popular approach of predicting representations, INRs <em>are</em> representations themselves: given $f(x)=y$, if we train a neural network $g_\theta$ to predict $y$ from $x$, the parameters of this network $\theta$ <em>implicitly represent</em> the function $f(\cdot)$.</p> <p>Hence, when learning several functions $f_1, \dots, f_n$, we can decide to predict weights (or INRs) for $f_i$, using a generating function $H(\cdot)$ as $H(i) = \theta_i$ such that $g_{\theta_i}(x) = f_i(x)$. From the deep learning perspective, this generating function $H(\cdot)$ is called a hypernetwork.</p> <p>This ability to conditionally generate INRs is what makes hypernetworks strongly applicable in physics-informed machine learning. In particular, we can see how the process is just another formuation of partial differential equations (PDEs), which denote time-varying functions of space $\nu_t(x)$ in differential form.</p> <p>Hypernetworks can very well predict spatial functions given a timestep: $H(t) = \theta_t$ such that $g_{\theta_t}(x) = \nu_t(x)$ as they can conditionally generate an INR corresponding to the function at tilmestep $t$, and can be used to forecast PDEs. Indeed they‚Äôve been used to do so in a very interesting <a href="https://arxiv.org/abs/2209.14855">NeurIPs paper</a> that I‚Äôve presented in a tutorial <a href="https://jaisidhsingh.github.io/assets/DINo_continuous_pde_forecasting_with_INRs.html">here</a>.</p> <p><br/></p> <h3 id="hypernetworks-as-quantifiers-of-functional-similarity">Hypernetworks as quantifiers of functional similarity</h3> <p>Let‚Äôs say that we wish to know the functional similarity between two encoders $f_A, f_B: \mathbb{R}^{m} \to \mathbb{R}^{d}$. The most straightforward way to do this would be to collect a stack of encodings for $N$ inputs for each encoder and use a similarity function like Centered-Kernel-Alignment (CKA) on these features: \(s_1 = CKA(O_A, O_B) \ ; \quad O_A, O_B \in \mathbb{R}^{N\times d}\) where $s_1 \in [0, 1]$ is a score denoting the functional similarity between the two models, and $O_A, O_B$ are the stacks of encoders for $N$ inputs.</p> <p>However, let‚Äôs think of another way to quantify the functional similarity between $f_A$ and $f_B$, specifically, by using a hypernetwork $H$ that predicts linear classifiers $W_A, W_B \in \mathbb{R}^{d\times k}$ on top of the two encoders $f_A, f_B$. Here, $k$ is the number of classes.</p> <p>Writing, the composite function $g_j(\bullet) = W_j f_j(\bullet), \ j \in {A, B}$ to classify the inputs, we outline the scheme that let‚Äôs the hypernetwork depict functional similarity between $f_A$ and $f_B$ as follows:</p> <ul> <li> <p>Given a dataset $\mathcal{D}$ and the current state of the hypernetwork‚Äôs parameters $\phi^0$, predict \(W^{0}_A \text{ , } W^{0}_B = H_{\phi^0}(c_A) \text{ , } H_{\phi^0}(c_B)\)</p> </li> <li>Obtain the classification loss of $g^0_A$ and $g^0_B$ on $\mathcal{D}$ as $\ell^0_A$ and $\ell^0_B$.</li> <li>Train the hypernetwork‚Äôs parameters $\phi$ only on encoder $f_A$ as \(\phi^1 \leftarrow \phi^0 - \eta \nabla_{\phi^0}\ell^0_A\)</li> <li>Then predict \(W^{1}_A \text{ , } W^{1}_B = H_{\phi^1}(c_A) \text{ , } H_{\phi^1}(c_B)\)</li> <li>Obtain the classification loss of $g^1_A, g^1_B \text{ as } \ell^1_A, \ell^1_B$.</li> <li>Then, the magnitude of $\Delta = \ell^0_B - \ell^1_B$ depicts the functional similarity between encoders $f_A$ and $f_B$.</li> </ul> <p>In other words, if training the hypernetwork <em>only</em> using network A lowers the loss of network B as well, then network A and B can be called functionally similar.</p> <hr/> <p>The above interpretations of hypernetworks are avenues that I think are rather under-utilised in the current literature, and can offer creative and potentially powerful ways of modulating neural networks. I‚Äôd be happy to know what you think of the above, or about parameter prediction in general. Hit my up for a chat on <a href="https://x.com/jaisidhsingh"> X/twitter</a> if you like.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Hypernetworks from the POV of INRs and functional similarity]]></summary></entry></feed>