<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jaisidhsingh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jaisidhsingh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-22T11:45:40+00:00</updated><id>https://jaisidhsingh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Ultra-Scale Playbook vol-1 - Single GPU</title><link href="https://jaisidhsingh.github.io/blog/" rel="alternate" type="text/html" title="Ultra-Scale Playbook vol-1 - Single GPU"/><published>2025-09-01T00:00:00+00:00</published><updated>2025-09-01T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/uspb1</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Optimally scaling LLM training across multiple GPUs requires finding the right tradeoffs between memory, computational efficiency, and communication overhead.</p> <p>It’s crucial to understand that each component, i.e., memory, compute-efficiency, and communication overhead, needs to be tuned carefully.</p> <h3 id="communication">Communication</h3> <p>Communication among nodes requires GPUs to be idle. That’s detrimental to overall efficiency because we want our GPUs to spend the most amount of time computing. That requires optimising intra-node/inter-node bandwidth usage, data transfers, and waiting for/syncing GPUs.</p> <h3 id="memory">Memory</h3> <p>Talking about memory, storing <em>all</em> activations is quadratically expensive with $seq$. Instead, store only expensive-to-recompute activations, discard the rest, and then recompute the discarded ones in the backward pass. We call this <strong>gradient checkpointing</strong> or <strong>recompilation</strong>, that trades off compute for memory</p> <p>Attention scores and matrices are an example of recomputable activations. <code class="language-plaintext highlighter-rouge">FlashAttention</code> natively integrates recomputing attention values/matrices in the backward pass out-of-the-box.</p> <p><strong>Consideration wrt GPU architecture:</strong> GPUs have limited high-speed memory, and accessing memory is typically slower than performing computations.</p> <h3 id="compute-efficiency">Compute-efficiency</h3> <p>Activations still scale linearly with $bs$, which is why we use <em>gradient accumulation</em>: sum up (avg in practice) gradients across $k$ passes (with batch size $bs/k$) and only then take an optimiser step. Requires keeping buffers for gradients that persist in a training step.</p> <p>However, gradient accumulation does not mean free lunch, as we get more computational overhead from lower per-pass batch size $bs/k$. Taking an optimiser step once every $k$ batches does not alleviate the cost of more forward &amp; backward passes.</p> <p>What’s neat is that we can parallelise the $k$ forward + backward passes. They aren’t correlated in any manner and just need to be summed to accumulate the gradient. However, that needs more VRAM.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Kick-off to a series of notes on LLM scaling]]></summary></entry><entry><title type="html">New POVs on hypernetworks</title><link href="https://jaisidhsingh.github.io/blog/" rel="alternate" type="text/html" title="New POVs on hypernetworks"/><published>2025-08-20T00:00:00+00:00</published><updated>2025-08-20T00:00:00+00:00</updated><id>https://jaisidhsingh.github.io/hypernets</id><content type="html" xml:base="https://jaisidhsingh.github.io/blog/"><![CDATA[<h3 id="tldr">TLDR</h3> <p><strong>hypernetworks can be interpreted to</strong></p> <ul> <li><strong>generate of implicit neural representations, and</strong></li> <li><strong>quantify of functional similarity between models</strong></li> </ul> <hr/> <p>I’ve worked on hypernetworks (neural networks that parameterize other neural networks) for a little while, that has led to <a href="https://openreview.net/forum?id=dyRHRxcgXX">a workshop paper at ICLR 2025</a> and <a href="https://arxiv.org/pdf/2507.10015">a main conference paper at EMNLP 2025</a>. While working on these papers, I’ve had the time to think about hypernetworks in a couple of fascinating ways that I’ve described below.</p> <h3 id="hypernetworks-as-generators-of-inrs">Hypernetworks as generators of INRs</h3> <p><em>Implicit neural representations</em> (INRs) are a clever and rather under-appreciated class of representations. Contrary to the popular approach of predicting representations, INRs <em>are</em> representations themselves: given $f(x)=y$, if we train a neural network $g_\theta$ to predict $y$ from $x$, the parameters of this network $\theta$ <em>implicitly represent</em> the function $f(\cdot)$.</p> <p>Hence, when learning several functions $f_1, \dots, f_n$, we can decide to predict weights (or INRs) for $f_i$, using a generating function $H(\cdot)$ as $H(i) = \theta_i$ such that $g_{\theta_i}(x) = f_i(x)$. From the deep learning perspective, this generating function $H(\cdot)$ is called a hypernetwork.</p> <p>This ability to conditionally generate INRs is what makes hypernetworks strongly applicable in physics-informed machine learning. In particular, we can see how the process is just another formuation of partial differential equations (PDEs), which denote time-varying functions of space $\nu_t(x)$ in differential form.</p> <p>Hypernetworks can very well predict spatial functions given a timestep: $H(t) = \theta_t$ such that $g_{\theta_t}(x) = \nu_t(x)$ as they can conditionally generate an INR corresponding to the function at tilmestep $t$, and can be used to forecast PDEs. Indeed they’ve been used to do so in a very interesting <a href="https://arxiv.org/abs/2209.14855">NeurIPs paper</a> that I’ve presented in a tutorial <a href="https://jaisidhsingh.github.io/assets/DINo_continuous_pde_forecasting_with_INRs.html">here</a>.</p> <h3 id="hypernetworks-as-quantifiers-of-functional-similarity">Hypernetworks as quantifiers of functional similarity</h3> <p>Let’s say that we wish to know the functional similarity between two encoders $f_A, $f_B: \mathbb{R}^{m} \to \mathbb{R}^{d}$. The most straightforward way to do this would be to collect a stack of encodings for $N$ inputs for each encoder and use a similarity function like Centered-Kernel-Alignment (CKA) on these features: \(s_1 = CKA(O_A, O_B) \ ; \quad O_A, O_B \in \mathbb{R}^{N\times d}\) where $s_1 \in [0, 1]$ is a score denoting the functional similarity between the two models, and $O_A, O_B$ are the stacks of encoders for $N$ inputs.</p> <p>However, let’s think of another way to quantify the functional similarity between $f_A$ and $f_B$, specifically, by using a hypernetwork $H$ that predicts linear classifiers $W_A, W_B \in \mathbb{R}^{d\times k}$ on top of the two encoders $f_A, f_B$. Here, $k$ is the number of classes.</p> <p>Writing, the composite function $g_j(\bullet) = W_j f_j(\bullet), \ j \in {A, B}$ to classify the inputs, we outline the scheme that let’s the hypernetwork depict functional similarity between $f_A$ and $f_B$ as follows:</p> <ul> <li>Given a dataset $\mathcal{D}$ and the current state of the hypernetwork’s parameters $\phi^0$, predict $W^0<em>A, W^0_B = H</em>{\phi^0}(c_A), H_{\phi^0}(c_B)$</li> <li>Obtain the classification loss of $g^0_A$ and $g^0_B$ on $\mathcal{D}$ as $\ell^0_A$ and $\ell^0_B$.</li> <li>Train the hypernetwork’s parameters $\phi$ only on encoder $f_A$ as \(\phi^1 \leftarrow \phi^0 - \eta \nabla_{\phi^0}\ell^0_A\)</li> <li>Predict $W^1<em>A, W^1_B = H</em>{\phi^1}(c_A), H_{\phi^1}(c_B)$ and obtain the classification loss of $g^1_A$ and $g^1_B$ as $\ell^1_A$ and $\ell^1_B$.</li> <li>Then, the magnitude of $\Delta = \ell^0_B - \ell^1_B$ depicts the functional similarity between encoders $f_A$ and $f_B$.</li> </ul> <p>In other words, if training the hypernetwork <em>only</em> using network A lowers the loss of network B as well, then network A and B can be called functionally similar.</p> <p>This is because the hypernetwork is able to parmaterise them well from a common state achieved using <em>only</em> network A.</p> <hr/> <p>The above interpretations of hypernetworks are avenues that I think are rather under-utilised in the current literature, and can offer creative and potentially powerful ways of modulating neural networks. I’d be happy to know what you think of the above, or about parameter prediction in general. Hit my up for a chat on <a href="https://x.com/jaisidhsingh"> X/twitter</a> if you like.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><summary type="html"><![CDATA[Hypernetworks from the POV of INRs and functional similarity]]></summary></entry></feed>