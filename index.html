<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jaisidh Singh</title>

    <meta name="author" content="Jaisidh Singh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jaisidh Singh
                </p>
                <p>
                  I am an upcoming MS in Machine Learning Student at University of Tubingen, Germany. I completed by B.Tech from <a href="https://iitj.ac.in/">IIT Jodhpur</a> majoring in <a>AI and Data Science</a>.
                  <br><br>
                  I work on a wide range of machine learning and deep learning problems. Specifically, I am most interested in robust <span style="color: salmon">vision-language reasoning and understanding</span> through <span style="color: seagreen">handcrafted algorithmic improvements as well as scaling up model capacity</span>. Besides this, my research and interests also touch upon <span style="color: blueviolet">interpretability and responsible AI</span>.
                  <!-- a researcher at <a href="http://iab-rubric.org/">Trusted AI Lab</a> with <a href="http://home.iitj.ac.in/~richa/">Dr. Richa Singh</a>, <a href="http://home.iitj.ac.in/~mvatsa/">Dr. Mayank Vatsa</a>, and <a href="https://www.aparnabharati.com/">Dr. Aparna Bharati</a>. Here, I have worked on <strong>privacy in face image generation</strong>, and my current research is on <strong>robust compositionality in VLMs</strong>.<br><br>  -->
                  <!-- I have also interned for <a href="https://www.bosch-ai.com/">Bosch AI Research India</a> where I worked with <a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/amit-kale/">Dr. Amit Arvind Kale</a>'s team on <strong>zero-shot detection and interpretation of systematic errors in segmentation</strong>. My work at Bosch has also touched upon <strong>inpainting with latent diffusion models, prompt analysis for multimodal image retrieval</strong>.<br><br>  -->
		              <!-- My aim is to work for multidisciplinary research labs at the forefront of AI research. My experience and interests lie at the intersection of <i style="color:seagreen ;">computer vision</i> and <i style="color: salmon;">responsible AI</i>.  -->
		              <!-- I am looking to work in these domains and to build fruitful, long-lasting collaborations, producing high-quality research work and publications. -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:singh.118@iitj.ac.in">Email</a> &nbsp;/&nbsp;
                  <a href="assets/cv_latest.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Z6fahScAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jaisidhsingh">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jaisidh-singh-66175a201/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jaisidhsingh/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="assets/jaisidh-passport-photo.jpeg">
              </td>
            </tr>
          </tbody></table>
          <!-- ================================================= RESEARCH================================ -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  <!-- I'm interested in <span style="color: crimson;">multimodal foundation models</span>, <span style="color: seagreen;">interpretability</span>, and <span style="color: blueviolet;">responsible AI</span>. My research aims at making deep learning models more reliable across their applications in computer vision. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- ============================================= PUBLICATIONS ================================================== -->
    <!-- CoN-CLIP -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/pubs/conclip.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:top">
          <br><span class="papertitle">Learn "No" To Say "Yes" Better: Improving Vision-Language Models via Negations</span>
        <br>
        <strong>Jaisidh Singh</strong>,
        <a href="">Ishaan Shrivastava</a>,
        <a href="">Mayank Vatsa</a>,
        <a href="">Richa Singh</a>,
        <a href="">Aparna Bharati</a>
        <br>
        <a href="https://arxiv.org/abs/2403.20312"> arXiv Preprint</a> / <a href="https://github.com/jaisidhsingh/CoN-CLIP">GitHub</a>
        <p></p>
        <p>
        VLMs like CLIP often ignore the effect of negation words such as "no", "not", etc. To solve this issue, we propose novel modifications to the CLIP loss using our high-quality dataset containing negations towards significant gains in image classification and general-purpose compositionality.
        </p>
      </td>
    </tr>



    <!-- SYNTHPROV -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/pubs/synthprov.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:top">
          <br><span class="papertitle">SynthProv: Interpretable Framework for Profiling Identity Leakage</span>
        <br>
        <strong>Jaisidh Singh</strong>,
        <a href="">Harshil Bhatia</a>,
        <a href="">Mayank Vatsa</a>,
        <a href="">Richa Singh</a>,
        <a href="">Aparna Bharati</a>
        <br>
        <em>WACV</em>, 2024
        <br>
        <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Singh_SynthProv_Interpretable_Framework_for_Profiling_Identity_Leakage_WACV_2024_paper.pdf">Paper</a>
        /
        <a href="https://drive.google.com/file/d/1RKdJTVw_gSOS-R2xAoaK9yQbGA1FmVyv/view?pli=1">Poster</a>
        /
        <a href="https://drive.google.com/file/d/1AinWiJWQW_Esj4_J_u-5q_xJFdv09ReT/view?pli=1">Slides</a>
        <p></p>
        <p>
        GANs remember identities seen during training and leak those features across their representations <i>(identity leakage)</i>. Our framework makes it possible to detect and trace identity leakage across synthetic images and the GAN's latent space.
        </p>
      </td>
    </tr>
	
    <!-- ID-PROV -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/pubs/idprov.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:100%;vertical-align:top">
          <br><span class="papertitle">IdProv: Identity-Based Provenance for Synthetic Image Generation (Student Abstract)</span>
        <br>
        <a href="">Harshil Bhatia*</a>,
        <strong>Jaisidh Singh*</strong>,
        <!-- <a href="">Gaurav Sangwan</a>, -->
        <a href="">Aparna Bharati</a>,
        <a href="">Richa Singh</a>,
        <a href="">Mayank Vatsa</a>
        <br>
        <em>AAAI</em>, 2023
        <br>
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26942">Paper</a>
        /
        <a href="https://drive.google.com/file/d/1HsnUiVML0f1_B-lR1dfchsKEWSHQ18kZ/view?usp=sharing">Poster</a>
        <p></p>
        <p>
          Highlights the motivation for identity leakage analysis and the threat it poses to privacy.
        </p>
      </td>
    </tr>
    </tbody></table>

<!-- ======================================== EXPERIENCE ========================================== -->
<!--   -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" >
    <tbody>
      <tr>
        <h2>Experience</h2>
      </tr>

      <br><br>

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:80px" src="assets/cerc.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Research Associate</strong></span><span style="float:right">May 2024 to Present</span> <br>
            <span><em><a href="http://iab-rubric.org/" target="_blank">MILA</a></em></span> <span style="float:right">Montreal, Canada</span>
            <br>
            <strong>With </strong><a href="https://digantamisra98.github.io/" target="_blank">Diganta Mishra</a>
            <br>
            <strong>Project Domain:</strong> Vision-Language Models, Alignment, Zero-shot Parameter Generation.
          </p>
        </td>
      </tr> 

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:80px" src="assets/cerc.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Research Associate</strong></span><span style="float:right">Jan 2024 to March 2024</span> <br>
            <span><em><a href="http://iab-rubric.org/" target="_blank">MILA</a></em></span> <span style="float:right">Montreal, Canada</span>
            <br>
            <strong>With </strong><a href="https://digantamisra98.github.io/" target="_blank">Diganta Mishra</a>
            <br>
            <strong>Project Domain:</strong> Continual learning, Mixture-of-Experts, Transformers
          </p>
        </td>
      </tr> 

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:120px" src="assets/iab.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Undergraduate Researcher</strong></span><span style="float:right">July 2023 - March 2024</span> <br>
            <span><em><a href="http://iab-rubric.org/" target="_blank">Trusted AI Lab, IIT Jodhpur</a></em></span> <span style="float:right">Rajasthan, India</span>
            <br>
            <strong>With </strong><a href="http://home.iitj.ac.in/~richa/">Dr. Richa Singh</a>, <a href="http://home.iitj.ac.in/~mvatsa/">Dr. Mayank Vatsa</a>, and <a href="https://www.aparnabharati.com/">Dr. Aparna Bharati</a>
            <br>
            <strong>Project Domain:</strong> Vision-Language models, Compositionality, Contrastive learning
            <br>
          </p>
        </td>
      </tr> 

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:120px" src="assets/bosch.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Computer Vision Research Engineer</strong></span><span style="float:right">May 2023 - Jan 2024</span> <br>
            <span><em><a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/amit-kale/" target="_blank">Bosch Research India</a></em></span> <span style="float:right">Madison, WI</span>
            <br>
            <strong>With </strong><a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/amit-kale/">Dr. Amit Arvind Kale, Sonam Singh.</a>
            <br>
            <strong>Project Domain:</strong> Systematic errors, Semantic segmentation, Zero-shot foundation models.
          </p>
        </td>
      </tr> 

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:120px" src="assets/iab.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Undergraduate Researcher</strong></span><span style="float:right">May 2022 - Jan 2023</span> <br>
            <span><em><a href="http://iab-rubric.org/" target="_blank">Trusted AI Lab, IIT Jodhpur</a></em></span> <span style="float:right">Rajasthan, India</span>
            <br>
            <strong>With </strong><a href="http://home.iitj.ac.in/~richa/">Dr. Richa Singh</a>, <a href="http://home.iitj.ac.in/~mvatsa/">Dr. Mayank Vatsa</a>, and <a href="https://www.aparnabharati.com/">Dr. Aparna Bharati</a>
            <br>
            <strong>Project Domain:</strong> Face-GANs, Latent space semantic encoding, identity leakage, privacy.
            <br>
          </p>
        </td>
      </tr> 

      <tr>
        <td width="20%" valign="center" align="center">
          <img style="width:120px" src="assets/bosch.png">
        </td>
        <td width="80%" valign="top">
          <p>
            <span><strong>Computer Vision Research Engineer</strong></span><span style="float:right">May 2022 - July 2022</span> <br>
            <span><em><a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/amit-kale/" target="_blank">Bosch Research India</a></em></span> <span style="float:right">Madison, WI</span>
            <br>
            <strong>With </strong>Sonam Singh.
            <br>
            <strong>Project Domain:</strong> Multimodal image retrieval, Vision transformers, Prompting for Vision-Language models.
          </p>
        </td>
      </tr> 
 
    </tbody></table>

<!-- ================================= PROJECTS ============================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" ><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
                <p>
                  Some of my self-projects can be found here.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" ><tbody>

    <!-- LORA CLIP -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/lora.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">LoRA-CLIP: Python Library</span>
        <br>
        <a href="https://github.com/jaisidhsingh/LoRA-CLIP">GitHub</a> / <a href="https://pypi.org/project/loraclip">PyPi</a>
        <p></p>
        <p>
          An easy-to-use wrapper over LoRA layers which can be inserted into CLIP for parameter-efficient fine-tuning. Offers direct LoRA-ification of the text encoder, image encoder, or both, at the rank of your choosing. Provides detailed trainable parameter counts at model load.
        </p>
      </td>
    </tr>

    <!-- QUICKMATCH -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/matchers.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">Quickmatch: Python Library</span>
        <br>
        <a href="https://github.com/jaisidhsingh/quickmatch">GitHub</a> / <a href="https://pypi.org/project/quickmatch">PyPi</a>
        <p></p>
        <p>
          A simple and fast commandline tool to create face-matcher embeddings out of any image folder specified. Supports 4 popular SOTA face-matchers/face-recognition networks: ElasticFace, ArcFace, SphereFace, and FaceNet.
        </p>
      </td>
    </tr>

    <!-- DIFFUSION PROJECT -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/diff.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">Latent Diffusion Inpainting</span>
        <br>
        <a href="https://github.com/jaisidhsingh/Diffusion-Tinkering">Code</a>
        <p></p>
        <p>
          A modular, plug-and-play organization of my tinkering with diffusion. Flexible training and inference for DDPMs & LDMs on Stanford Cars, LSUN Dining Rooms, and Stable Diffusion inference. Adds grounded inpainting + generation Stable Diffusion and GLIGEN.
        </p>
      </td>
    </tr>

    <!-- UVSUMM -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/uvsumm.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">UV-Summ: Transformer U-Net for Video Summarization</span>
        <br>
        <a href="https://github.com/jaisidhsingh/UVSumm-Video">Code</a>
        <p></p>
        <p>
         An architecture inspired by UNet for scoring CLIP frame features to extract key-frames. Includes a zero-shot text summarizer using CLIP embeddings of the extracted key-frames. Our results outperformed recent approaches on the TvSumm datasets, and this was presented as the course project for Deep Learning 2023 @ IIT Jodhpur
        </p>
      </td>
    </tr>

    <!-- CLUSTER_SUMM -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/multisumm.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">Beyond Token Limits: Inference-time Optimization for Large Document Summarization</span>
        <br>
        <a href="https://github.com/jaisidhsingh/UVSumm-Video">Code</a>
        /
        <a href="https://huggingface.co/spaces/jaisidhsingh/cluster-summ">Deployed</a>
        <p></p>
        <p>
        A high-utility project for the summarization of large articles, in a purely inference-based, plug-and-play manner. Utilizing hierarchical sentence clustering for extractive summarization, this was presented as the DL-Ops project for DL 2023 @ IIT Jodhpur.
        </p>
      </td>
    </tr>

    <!-- LOWRES -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/lowres.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">LowResFormer: A Transformer for Low Resolution Fine Grained Image Classification</span>
        <br>
        <a href="https://github.com/jaisidhsingh/LowResFormer-ViT">Code</a>
        <!-- / -->
        <!-- <a href="https://huggingface.co/spaces/jaisidhsingh/cluster-summ">Deployed</a> -->
        <p></p>
        <p>
        A vision transformer architecture developed as my first self-research endeavour. Utilized multimodal inputs of images along with their attribute information, to classify images. Outperformed previous approached on the AwA2 dataset, in all resolutions.
        </p>
      </td>
    </tr>

    <!-- RL -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='assets/projects/dqn.png' width=100%>
        </div>
      </td>
      <td style="padding:0px;width:75%;vertical-align:text-bottom">
          <br><span class="papertitle">Deep RL: Agents on Gym and Custom Environments</span>
        <br>
        <a href="https://github.com/jaisidhsingh/Deep-Reinforcement-Learning">Code</a>
        <!-- / -->
        <!-- <a href="https://huggingface.co/spaces/jaisidhsingh/cluster-summ">Deployed</a> -->
        <p></p>
        <p>
        A project of self-implemented RL algorithms on OpenAI's Gym to gain an understanding of this field. Implemented the DQN and Permutation Invariant Senory Transformer papers, and used this learning to apply DQN for the Course Project for Advanced Artificial Intelligence 2023 @ IIT Jodhpur.
        </p>
      </td>
    </tr>
    </tbody></table>
  
          <!-- EXTRAS  -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Extras</h2>
                <p>
               I am an avid reader, and enjoy fiction and non-fiction, especially mythology and philosophy.
	I am also an experienced guitarist and vocalist. I have performed at large-scale events like 
	<a href="https://ignus.co.in/">IGNUS</a> and <a href="https://www.instagram.com/interiit_culturals/">Inter-IIT Cultural Meet</a>. As a Core Member and Mentor of
	<a href="https://www.instagram.com/sangam_iitj/?hl=en">Sangam Music Society @ IIT Jodhpur</a> I have guided 3 batches of musicians of the society. 
   Personally, I am drawn to collaborations and leadership roles,
	and positions which allow me to help people. Being a <a href="http://swc.iitj.ac.in/">Student Guide @ SWC IITJ (2021)</a> allowed  
	me to closely mentor 10 students, and handle a junior batch of 500 students
	with a team of 45. My varied interests and skills also made me a Core Member (2021) of <a href="https://www.instagram.com/quizsociitj/">Quiz Society</a>, 
	<a href="https://www.instagram.com/litsociitj/">Literature Society</a>, and <a href="https://devluplabs.tech/#/">DevlUp Labs @ IITJ</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- CITE JON BARRON -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding-left:20px">
                <br>
                <p style="text-align: left;font-size:xx-small;vertical-align: middle;">
                  Website template taken from <a style="font-size: xx-small;" href="https://github.com/jonbarron/jonbarron_website">Jon Barron's github repository</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
<!--  -->
