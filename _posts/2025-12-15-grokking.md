---
layout: post
title: Grokking fast and slow
date: 2025-12-15
description: Whats, whys, and what-ifs of grokking 
tags: ml
categories: ml
featured: false
---

### TLDR

Grokking, or delayed generalisation in neural networks, can
- help us understand when and how (potentially dangerous) capabilities can emerge in neural networks
- circumvent data limitations by showing that the right inductive biases can lead to strong networks
- be *partially* explained by current theories of lazy-to-feature learning, however, exceptions exist as shown below.

### What is grokking?

Grokking is when a neural network generalises, i.e, achieves zero test error, significantly after already achieving near zero train error (see figure below). 
This phenomena has been extensively studied in small networks on toy math problems, however, it's not restricted to these settings as shown by <a href="">this paper</a>.

### Why grokking happens: weight decay v/s lazy-to-feature learning

Why a network groks on a particular task is still a mystery, although a number of studies have tried to answer this. Popularly, one collection of these studies approached grokking from the perspective of weight decay. When the train loss is nearly zero, parameter updates are largely driven by weight decay, which still allow significant exploration in the parameters to induce the sudden discovery of the rule (the steep ascent). Newer works along these lines have also formulated grokking as weight-norm minimisation in the post-memorization phase.

Another theory, however, has emerged which models grokking as the transition from a "lazy-like" learning regime to feature learning regime. More specifically, it argues that in the memorization phase, the network learns lazily, i.e, its neural tangent kernel (NTK) evolves rather negligbly until the network discovers the generalisable rule, after which the network enters a "rich feature learning" phase where the NTK evolves quite variably.

While both theories present compelling evidence for their core hypotheses, one cannot explain grokking *in every architecture and task* using them. Grokking profiles can vary quite strongly depending on one's model, task, and optimisation setup, and we can see counter-examples to the claims of each theory (weight decay and lazy-to-feature learning) below.

### Grokking can be seen without weight decay

asdf

### Grokking can be seen in purely feature learning regimes

asdf

