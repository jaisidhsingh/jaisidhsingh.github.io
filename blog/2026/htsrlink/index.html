<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Simple Toy Model Bridging HTSR &amp; Alpha-REQ | Jaisidh Singh </title> <meta name="author" content="Jaisidh Singh"> <meta name="description" content="Bounding conditioning of features with that of weights"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/me.jpg?4ea4f62b15165b7e17cff5fc29aec32c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaisidhsingh.github.io/blog/2026/htsrlink/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jaisidh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Simple Toy Model Bridging HTSR &amp; Alpha-REQ</h1> <p class="post-meta"> Created on February 10, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tldr">TLDR</h2> <blockquote> <p>Generalization in neural networks has been shown to be correlated with one phenomenon expressed in two venues: how strongly singular values decay in (i) weights (Martin and Mahoney’s HTSR theory) and (ii) features (Agarwal’s $\alpha$-REQ). This blog connects the two approaches for toy linear models to show how HTSR may give rise to $\alpha$-REQ.</p> </blockquote> <p><br></p> <h2 id="introduction">Introduction</h2> <p>Recent advances in deep learning theory have used spectral analysis as a tool to study generalization of neural networks. On one hand, we have Heavy-Tailed Self-Regularization (HTSR) theory, which analyzes a network’s weights to demonstrate that in well-trained networks, singular values of weights follow a power-law decay. If the powerlaw coefficient is less than 2, that signifies over-fitting. In contrast, powelaw coefficients between 2 and 4 signify a well generalised state and above 6 denotes under-trained weight. On the other hand, $\alpha$-ReQ analyzes activations or features to show that high-quality representations also follow a specific spectral decay that correlates with generalization, i.e., powerlaw fit of less than 1.5 denotes over-fitting while 1.5 to 2 denotes general features.</p> <p>While the two approaches are proposed and used separately, it is plain that properties of weights and features are coupled by design. Hence, this post explores the connection between HTSR and $\alpha$-REQ through fundamental linear algebra on a toy linear model $Y = XW$.</p> <ol> <li>First, we will quickly relate the condition number of a matrix to the decay of the singular values.</li> <li>Then we will use our linear model to look at how the singular value decay/conditioning of the weights $W$ can impact the features $Y$.</li> </ol> <p>The result in this post is mainly a corollary of those in the vast literature on signal propagation in neural networks. Our aim here is to simply yet mathematically relate two interesting emergences of geometry in distinct spaces, i.e., weights and features.</p> <p><br></p> <h2 id="linking-singular-value-decay-to-condition-number">Linking singular value decay to condition number</h2> <p>Intuitively, one can see that a steeper decay in the singular value spectrum of a matrix should correspond to a higher condition number and vice versa, as a flatter spectrum implies that the minimum singular value is still reasonably close to the maximum singular value. Let us prove this formally.</p> <p>We start with a matrix $A \in \mathbb{R}^{n \times m}$ that has full rank, i.e., $A^T A$ is symmetric and positive semi-definite (PSD). Then, we can write the condition number of $A$ as</p> \[\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = \frac{\sqrt{\lambda_{\max}(A^T A)}}{\sqrt{\lambda_{\min}(A^T A)}}\] <p>Then, we fit to the eigenvalues of $A^T A$ a powerlaw such that $\lambda_k = c \cdot k^{-\alpha}$ where $k$ is the index of the eigenvalue in the spectrum (in descending oder) and $c$ is a fixed scalar. Note that since $A$ is rull rank, the number of non-zero eigenvalues of $A^T A$ is $m$, which is also its rank. Accordingly, one can say that if $\lambda_{\max} = c$ then $\lambda_{\min} = c \cdot (m)^{-\alpha}$</p> <p>Then, we can substitute this in the expression for $\kappa(A)$ to find</p> \[\kappa(A) = \frac{\sqrt{\lambda_{\max}(A^T A)}}{\sqrt{\lambda_{\min}(A^T A)}} = \frac{\sqrt{c}}{\sqrt{c (m)^{-\alpha}}} = m^{\alpha/2}\] <p>This result confirms our intuition: $\kappa(A)$ is indeed proportional to how fast the singular value spectrum of $A$ decays. In the following, we will use this result to connect HTSR with $\alpha$-REQ cleanly.</p> <p><br></p> <h2 id="toy-model-to-bridge-htsr-and-alpha-req">Toy model to bridge HTSR and $\alpha$-REQ</h2> <p>Given a data matrix $X \in \mathbb{R}^{n \times d}$, where $n$ is the number of data points ad $d$ is the dimension of each sample’s features, we are interested in a linear transform of $X$ by weights $W \in \mathbb{R}^{d \times m}$ to compute $Y \in \mathbb{R}^{n \times m}$ as</p> \[Y = X W\] <p>According to HTSR theory, the powerlaw fit of the decay of singular values of $W$ should reduce as the weights are updated throughout training, and $\alpha$-REQ shows how the same happens for features $Y$. We will now derive a link between the two observations.</p> <p>For any two compatible matrices $A$ and $B$, the condition number of their matrix multiplication is bounded as</p> \[\kappa(AB) \leq \kappa(A) \cdot \kappa(B)\] <p>This result simply follows from the nice properties of singular values: given some singular values arranged in descending order like $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_N \geq 0$, we have for $A$ and $B$</p> \[\sigma_N(A)\sigma_i(B) \leq \sigma_i(AB) \leq \sigma_1(A)\sigma_i(B) \quad i=1,2,\dots,N\] <p>Corresondingly, $\sigma_{\max}(AB) \leq \sigma_{\max}(A) \ \sigma_{\max}(B)$ and $\sigma_{\text{min}}(AB) \geq \sigma_{\text{min}}(A) \ \sigma_{\text{min}}(B)$ give the above result for the condition number of a matrix multiplication. Then, we can see that</p> \[\kappa(Y) \leq \kappa(X) \cdot \kappa(W) = C_{\text{data}} \cdot \kappa(W)\] <p>Here $\kappa(X)$ is denoted by a fixed scalar $C_{\text{data}}$ that assumes the data matrix to be of full rank and fixed. Finally, using the relationship between condition number and powerlaw fit coefficient for singular value decay, we can see the bound</p> \[{r_Y}^{\alpha_Y/2} \leq (C_{\text{data}}) \cdot {r_W}^{\alpha_W/2} \implies \alpha_Y \leq 2\log_{r_Y}(C_{\text{data}}) + \alpha_W \log_{r_Y}(r_W)\] <p>Here, $r_Y$ and $r_W$ are the ranks of $Y^T Y$ and $W^T W$. If both $Y$ and $W$ are full rank, then $r_Y = r_W = m$.</p> <p>This shows that as the powerlaw fit of the singular value decay $\alpha_W$ for weights $W$ reduces, so should the maximum value of the corresponding powerlaw fit $\alpha_Y$ for features $Y$.</p> <p><br></p> <h2 id="discussion-on-deeper-models">Discussion on deeper models</h2> <p>It is not difficult to see what would happen for a 2-layer network with the following architecture:</p> \[Y = XW_1 \ ; \ \widetilde{Y} = \varphi(Y) \ ; \ Z = \widetilde{Y}W_2\] \[X \in \mathbb{R}^{n\times d}, W_1 \in \mathbb{R}^{d\times m}, Y \in \mathbb{R}^{n\times m}, \widetilde{Y} \in \mathbb{R}^{n\times m}, W_2 \in \mathbb{R}^{m \times c}, Z \in \mathbb{R}^{n\times c}\] <p>and $\varphi(\bullet)$ is an element-wise non-linear activation function such as $\text{ReLU}(\bullet)$. Now while we have derived a relation between the conditioning/singular value decay of $Y$ and $W_1$ above, note that there does not exist any definitive analytical link from $\kappa(Y)$ to $\kappa(Z)$. This is because of the non-linearity $\varphi(\bullet)$, which changes the conditioning of $Y$ rather opaquely. However, it is not the case that we cannot make any comment on the conditioning of $Z$. This is shown as follows.</p> <p>Since ReLU-like non-linearities zero-out negative inputs, we cannot know $\text{rank}(\widetilde{Y}^\top \widetilde{Y})$ without explicitly measuring it. Let us denote it by $r_{\widetilde{Y}}$. Then, using the bound on condition number of the product of two matrices, we now write</p> \[\kappa(Z) \leq \kappa(\widetilde{Y}) \cdot \kappa(W_2) \implies r_Z^{\alpha_Z/2} \leq r_{\widetilde{Y}}^{\alpha_{\widetilde{Y}}/2} \cdot r_{W_2}^{\alpha_{W_2}/2}\] <p>Taking log w.r.t. $r_Z$ on both sides, we get</p> \[\alpha_Z \leq \alpha_{\widetilde{Y}}\cdot \log_{r_Z}(r_{\widetilde{Y}}) + \alpha_{W_2} \cdot \log_{r_Z}(r_{W_2})\] <p>This means that the representation quality of the output is bounded by the representation quality of the input and how “well-trained” the weight operand is. In contrast, input layers only depend on the quality of the weight. Particularly, since ReLU can reduce steepness of the singular value decay, it can be possible that high-quality input features can prevent the quality of output features from collapsing, even under under-trained weights.</p> <p><br></p> <h2 id="why-should-we-care-about-this">Why should we care about this?</h2> <p>While the above result is no ground-breaking insight, it is in fact a nice sanity check: quality of input features and weights can make or break the quality of output features. In terms of applications, it can be interesting to think about interventions that mainly address features, yet remain crucially shaped by weight. Particularly, the result above can make one ponder about <em>steering</em> of language models. In steering, one adds learnable vectors at certain features within the network in order to modify or steer the network’s predictions. Given a query for which we want to optimize our network via steering, we would like to intervene on high-quality features: which encode rich, general concept modes relevant to our query. In contrast, features that have collapsed to only a few concept modes due to over-fitting would be hard to steer. Also, knowing the conditioning/trained-ness of the weights that would operate on chosen features can allow insight into how the effect of our steering propagates through the network. For instance, ill-conditioned weights can likely suppress directions that can be potentially relevant to optimizing the query. However, this remains conjecture at present, requiring experimental validation and deeper analysis that we keep for future work.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/muon/">Muon and Manifold Versions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/grokking/">Grokking fast and slow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/celebration/">Celebration is the secret</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/hypernets/">New POVs on hypernetworks</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Jaisidh Singh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>