<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> New POVs on hypernetworks | Jaisidh Singh </title> <meta name="author" content="Jaisidh Singh"> <meta name="description" content="Hypernetworks from the POV of INRs and functional similarity"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/me.jpg?4ea4f62b15165b7e17cff5fc29aec32c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaisidhsingh.github.io/blog/2025/hypernets/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jaisidh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">New POVs on hypernetworks</h1> <p class="post-meta"> Created on August 20, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="tldr">TLDR</h3> <blockquote> <p><strong>hypernetworks can be interpreted to</strong></p> <ul> <li><strong>generate implicit neural representations, and</strong></li> <li><strong>quantify functional similarity between models</strong></li> </ul> </blockquote> <hr> <p>I’ve worked on hypernetworks (neural networks that parameterize other neural networks) for a little while, that has led to <a href="https://openreview.net/forum?id=dyRHRxcgXX" rel="external nofollow noopener" target="_blank">a workshop paper at ICLR 2025</a> and <a href="https://arxiv.org/pdf/2507.10015" rel="external nofollow noopener" target="_blank">a main conference paper at EMNLP 2025</a>. While working on these papers, I’ve had the time to think about hypernetworks in a couple of fascinating ways that I’ve described below.</p> <h3 id="hypernetworks-as-generators-of-inrs">Hypernetworks as generators of INRs</h3> <p><em>Implicit neural representations</em> (INRs) are a clever and rather under-appreciated class of representations. Contrary to the popular approach of predicting representations, INRs <em>are</em> representations themselves: given $f(x)=y$, if we train a neural network $g_\theta$ to predict $y$ from $x$, the parameters of this network $\theta$ <em>implicitly represent</em> the function $f(\cdot)$.</p> <p>Hence, when learning several functions $f_1, \dots, f_n$, we can decide to predict weights (or INRs) for $f_i$, using a generating function $H(\cdot)$ as $H(i) = \theta_i$ such that $g_{\theta_i}(x) = f_i(x)$. From the deep learning perspective, this generating function $H(\cdot)$ is called a hypernetwork.</p> <p>This ability to conditionally generate INRs is what makes hypernetworks strongly applicable in physics-informed machine learning. In particular, we can see how the process is just another formuation of partial differential equations (PDEs), which denote time-varying functions of space $\nu_t(x)$ in differential form.</p> <p>Hypernetworks can very well predict spatial functions given a timestep: $H(t) = \theta_t$ such that $g_{\theta_t}(x) = \nu_t(x)$ as they can conditionally generate an INR corresponding to the function at tilmestep $t$, and can be used to forecast PDEs. Indeed they’ve been used to do so in a very interesting <a href="https://arxiv.org/abs/2209.14855" rel="external nofollow noopener" target="_blank">NeurIPs paper</a> that I’ve presented in a tutorial <a href="https://jaisidhsingh.github.io/assets/DINo_continuous_pde_forecasting_with_INRs.html">here</a>.</p> <h3 id="hypernetworks-as-quantifiers-of-functional-similarity">Hypernetworks as quantifiers of functional similarity</h3> <p>Let’s say that we wish to know the functional similarity between two encoders $f_A, f_B: \mathbb{R}^{m} \to \mathbb{R}^{d}$. The most straightforward way to do this would be to collect a stack of encodings for $N$ inputs for each encoder and use a similarity function like Centered-Kernel-Alignment (CKA) on these features: \(s_1 = CKA(O_A, O_B) \ ; \quad O_A, O_B \in \mathbb{R}^{N\times d}\) where $s_1 \in [0, 1]$ is a score denoting the functional similarity between the two models, and $O_A, O_B$ are the stacks of encoders for $N$ inputs.</p> <p>However, let’s think of another way to quantify the functional similarity between $f_A$ and $f_B$, specifically, by using a hypernetwork $H$ that predicts linear classifiers $W_A, W_B \in \mathbb{R}^{d\times k}$ on top of the two encoders $f_A, f_B$. Here, $k$ is the number of classes.</p> <p>Writing, the composite function $g_j(\bullet) = W_j f_j(\bullet), \ j \in {A, B}$ to classify the inputs, we outline the scheme that let’s the hypernetwork depict functional similarity between $f_A$ and $f_B$ as follows:</p> <ul> <li>Given a dataset $\mathcal{D}$ and the current state of the hypernetwork’s parameters $\phi^0$, predict $W^{0}<em>A$, $W^{0}_B$ = $H</em>{\phi^0}(c_A)$, $H_{\phi^0}(c_B)$</li> <li>Obtain the classification loss of $g^0_A$ and $g^0_B$ on $\mathcal{D}$ as $\ell^0_A$ and $\ell^0_B$.</li> <li>Train the hypernetwork’s parameters $\phi$ only on encoder $f_A$ as \(\phi^1 \leftarrow \phi^0 - \eta \nabla_{\phi^0}\ell^0_A\)</li> <li>Predict $W^{1}<em>A, W^{1}_B = H</em>{\phi^1}(c_A), H_{\phi^1}(c_B)$.</li> <li>Obtain the classification loss of $g^1_A, g^1_B \text{ as } \ell^1_A, \ell^1_B$.</li> <li>Then, the magnitude of $\Delta = \ell^0_B - \ell^1_B$ depicts the functional similarity between encoders $f_A$ and $f_B$.</li> </ul> <p>In other words, if training the hypernetwork <em>only</em> using network A lowers the loss of network B as well, then network A and B can be called functionally similar.</p> <hr> <p>The above interpretations of hypernetworks are avenues that I think are rather under-utilised in the current literature, and can offer creative and potentially powerful ways of modulating neural networks. I’d be happy to know what you think of the above, or about parameter prediction in general. Hit my up for a chat on <a href="https://x.com/jaisidhsingh" rel="external nofollow noopener" target="_blank"> X/twitter</a> if you like.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/celebration/">Celebration is the secret</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/uspb3/">Ultra-Scale Playbook vol-3 - DeepSpeed ZeRO</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/uspb2/">Ultra-Scale Playbook vol-2 - Data Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/uspb1/">Ultra-Scale Playbook vol-1 - Single GPU</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jaisidh Singh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>